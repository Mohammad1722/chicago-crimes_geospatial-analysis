{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebook is used to preprocess the data as chunk because the original dataset is too large and can't be loaded and processed in one-go in Pandas. The output is then pickled and stored next to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Names:  \n",
    "1. g\n",
    "1. ID\n",
    "1. Case Number\n",
    "1. Date\n",
    "1. Block\n",
    "1. IUCR\n",
    "1. Primary Type\n",
    "1. Description\n",
    "1. Location Description\n",
    "1. Arrest\n",
    "1. Domestic\n",
    "1. Beat\n",
    "1. District\n",
    "1. Ward\n",
    "1. Community Area\n",
    "1. FBI Code\n",
    "1. X Coordinate\n",
    "1. Y Coordinate\n",
    "1. Year\n",
    "1. Updated On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunk #001..\n",
      "processing chunk #002..\n",
      "processing chunk #003..\n",
      "processing chunk #004..\n",
      "processing chunk #005..\n",
      "processing chunk #006..\n",
      "processing chunk #007..\n",
      "processing chunk #008..\n",
      "processing chunk #009..\n",
      "processing chunk #010..\n",
      "processing chunk #011..\n",
      "processing chunk #012..\n",
      "processing chunk #013..\n",
      "processing chunk #014..\n",
      "processing chunk #015..\n",
      "processing chunk #016..\n",
      "processing chunk #017..\n",
      "processing chunk #018..\n",
      "processing chunk #019..\n",
      "processing chunk #020..\n",
      "processing chunk #021..\n",
      "processing chunk #022..\n",
      "processing chunk #023..\n",
      "processing chunk #024..\n",
      "processing chunk #025..\n",
      "processing chunk #026..\n",
      "processing chunk #027..\n",
      "processing chunk #028..\n",
      "processing chunk #029..\n",
      "processing chunk #030..\n",
      "processing chunk #031..\n",
      "processing chunk #032..\n",
      "processing chunk #033..\n",
      "processing chunk #034..\n",
      "processing chunk #035..\n",
      "processing chunk #036..\n",
      "processing chunk #037..\n",
      "processing chunk #038..\n",
      "processing chunk #039..\n",
      "processing chunk #040..\n",
      "processing chunk #041..\n",
      "processing chunk #042..\n",
      "processing chunk #043..\n",
      "processing chunk #044..\n",
      "processing chunk #045..\n",
      "processing chunk #046..\n",
      "processing chunk #047..\n",
      "processing chunk #048..\n",
      "processing chunk #049..\n",
      "processing chunk #050..\n",
      "processing chunk #051..\n",
      "processing chunk #052..\n",
      "processing chunk #053..\n",
      "processing chunk #054..\n",
      "processing chunk #055..\n",
      "processing chunk #056..\n",
      "processing chunk #057..\n",
      "processing chunk #058..\n",
      "processing chunk #059..\n",
      "processing chunk #060..\n",
      "processing chunk #061..\n",
      "processing chunk #062..\n",
      "processing chunk #063..\n",
      "processing chunk #064..\n",
      "processing chunk #065..\n",
      "processing chunk #066..\n",
      "processing chunk #067..\n",
      "processing chunk #068..\n",
      "processing chunk #069..\n",
      "processing chunk #070..\n",
      "processing chunk #071..\n",
      "processing chunk #072..\n",
      "processing chunk #073..\n",
      "processing chunk #074..\n",
      "processing chunk #075..\n",
      "processing chunk #076..\n",
      "processing chunk #077..\n",
      "processing chunk #078..\n",
      "processing chunk #079..\n",
      "processing chunk #080..\n",
      "processing chunk #081..\n",
      "processing chunk #082..\n",
      "processing chunk #083..\n",
      "processing chunk #084..\n",
      "processing chunk #085..\n",
      "processing chunk #086..\n",
      "processing chunk #087..\n",
      "processing chunk #088..\n",
      "processing chunk #089..\n",
      "processing chunk #090..\n",
      "processing chunk #091..\n",
      "processing chunk #092..\n",
      "processing chunk #093..\n",
      "processing chunk #094..\n",
      "processing chunk #095..\n",
      "processing chunk #096..\n",
      "processing chunk #097..\n",
      "processing chunk #098..\n",
      "processing chunk #099..\n",
      "processing chunk #100..\n",
      "processing chunk #101..\n",
      "processing chunk #102..\n",
      "processing chunk #103..\n",
      "processing chunk #104..\n",
      "processing chunk #105..\n",
      "processing chunk #106..\n",
      "processing chunk #107..\n",
      "processing chunk #108..\n",
      "processing chunk #109..\n",
      "processing chunk #110..\n",
      "processing chunk #111..\n",
      "processing chunk #112..\n",
      "processing chunk #113..\n",
      "processing chunk #114..\n",
      "processing chunk #115..\n",
      "processing chunk #116..\n",
      "processing chunk #117..\n",
      "processing chunk #118..\n",
      "processing chunk #119..\n",
      "processing chunk #120..\n",
      "processing chunk #121..\n",
      "processing chunk #122..\n",
      "processing chunk #123..\n",
      "processing chunk #124..\n",
      "processing chunk #125..\n",
      "processing chunk #126..\n",
      "processing chunk #127..\n",
      "processing chunk #128..\n",
      "processing chunk #129..\n",
      "processing chunk #130..\n",
      "processing chunk #131..\n",
      "processing chunk #132..\n",
      "processing chunk #133..\n",
      "processing chunk #134..\n",
      "processing chunk #135..\n",
      "processing chunk #136..\n",
      "processing chunk #137..\n",
      "processing chunk #138..\n",
      "processing chunk #139..\n",
      "processing chunk #140..\n",
      "processing chunk #141..\n",
      "processing chunk #142..\n",
      "processing chunk #143..\n"
     ]
    }
   ],
   "source": [
    "# RUNS IN 20 MINS\n",
    "dfChunks_list = []\n",
    "chunkReader = pd.read_json(\"../data/chicagoCrimes/Chicago_Crimes.json\", lines=True, chunksize=50_000)\n",
    "for i, dfChunk in enumerate(chunkReader):\n",
    "    # preprocessing each chunk\n",
    "    print(f'processing chunk #{(i+1):03d}..')\n",
    "    \n",
    "    # remove entries without coordinates to avoid plotting problems\n",
    "    dfChunk.dropna(subset=['X Coordinate', 'Y Coordinate'], inplace=True)\n",
    "    \n",
    "    # remove unused columns to save space\n",
    "    dfChunk.drop(\n",
    "        columns=['Beat', 'Ward', 'Updated On', 'IUCR', 'Case Number', \n",
    "        'X Coordinate', 'Y Coordinate', 'Community Area', 'Block', 'Year'], \n",
    "        inplace=True)\n",
    "\n",
    "    # convert binary columns from string to int8 (maybe switch to bool later?)\n",
    "    dfChunk['Arrest'] = dfChunk['Arrest'].map({'true': 1, 'false':0}).astype(\"Int8\")\n",
    "    dfChunk['Domestic'] = dfChunk['Domestic'].map({'true': 1, 'false':0}).astype(\"Int8\")\n",
    "\n",
    "    # assign a score to each crime\n",
    "    dfChunk['Score Crime'] = 27 - pd.to_numeric(dfChunk['FBI Code'].str[:2]).astype(\"Int8\")\n",
    "    dfChunk['Score Arrest'] = (dfChunk['Arrest'] * 2).astype(\"Int8\")\n",
    "\n",
    "    # convert numerical datatypes to fitting containers to save space\n",
    "    dfChunk[['District']] = dfChunk[['District']].astype('Int8')\n",
    "    dfChunk[['ID']] = dfChunk[['ID']].astype('Int32')\n",
    "\n",
    "    # convert string types to categorical to save space\n",
    "    dfChunk[['Primary Type', 'Description', 'Location Description', 'District', 'FBI Code']] = dfChunk[['Primary Type', 'Description', 'Location Description', 'District', 'FBI Code']].astype(\"category\")\n",
    "\n",
    "    # convert numerical datatypes to fitting containers to save space\n",
    "    dfChunk['g'] = gpd.GeoSeries.from_wkt(dfChunk['g'])\n",
    "\n",
    "    # convert dates to datetime objects\n",
    "    dfChunk['Date'] = pd.to_datetime(dfChunk['Date'])\n",
    "\n",
    "    dfChunks_list.append(dfChunk)\n",
    "\n",
    "del dfChunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the processed chunks into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfChunks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-apply category type to specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Primary Type', 'Description', 'Location Description', 'District', 'FBI Code']] = df[['Primary Type', 'Description', 'Location Description', 'District', 'FBI Code']].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7078918 entries, 0 to 7147876\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   g                     geometry      \n",
      " 1   ID                    Int32         \n",
      " 2   Date                  datetime64[ns]\n",
      " 3   Primary Type          category      \n",
      " 4   Description           category      \n",
      " 5   Location Description  category      \n",
      " 6   Arrest                Int8          \n",
      " 7   Domestic              Int8          \n",
      " 8   District              category      \n",
      " 9   FBI Code              category      \n",
      " 10  Score Crime           Int8          \n",
      " 11  Score Arrest          Int8          \n",
      "dtypes: Int32(1), Int8(4), category(5), datetime64[ns](1), geometry(1)\n",
      "memory usage: 297.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../data/chicagoCrimes/Chicago_Crimes_cleaned.pkl\"\n",
    "df.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code needed to load the dataframe in any notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 7078918 entries, 0 to 7147876\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   g                     geometry      \n",
      " 1   ID                    Int32         \n",
      " 2   Date                  datetime64[ns]\n",
      " 3   Primary Type          category      \n",
      " 4   Description           category      \n",
      " 5   Location Description  category      \n",
      " 6   Arrest                Int8          \n",
      " 7   Domestic              Int8          \n",
      " 8   District              category      \n",
      " 9   FBI Code              category      \n",
      " 10  Score Crime           Int8          \n",
      " 11  Score Arrest          Int8          \n",
      "dtypes: Int32(1), Int8(4), category(5), datetime64[ns](1), geometry(1)\n",
      "memory usage: 297.0 MB\n"
     ]
    }
   ],
   "source": [
    "# dff = pd.read_pickle(file_name)\n",
    "# dff = gpd.GeoDataFrame(dff, geometry='g')\n",
    "# dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gsoc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122495c4063850a28ae774560ba5db19359a25c6781796f476c8a8f81f528aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
